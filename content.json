{"meta":{"title":"Study4Fun","subtitle":"学而时习之","description":"","author":"Mii Wang","url":"https://study4.fun","root":"/"},"pages":[{"title":"","date":"2021-04-28T13:38:44.833Z","updated":"2021-04-28T13:38:44.833Z","comments":true,"path":"talk-auto-init.js","permalink":"https://study4.fun/talk-auto-init.js","excerpt":"","text":"const fs = require('fs'); const path = require('path'); const url = require('url'); const request = require('request'); const xmlParser = require('xml-parser'); const md5 = require('md5'); // 配置信息 const config = { username: 'MiiW', // GitHub repository 所有者，可以是个人或者组织。对应Gitalk配置中的owner repo: \"gitalk\", // 储存评论issue的github仓库名，仅需要仓库名字即可。对应 Gitalk配置中的repo token: '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', // 前面申请的 personal access token sitemap: path.join(__dirname, './public/sitemap.xml'), // 自己站点的 sitemap 文件地址 cache: true, // 是否启用缓存，启用缓存会将已经初始化的数据写入配置的 gitalkCacheFile 文件，下一次直接通过缓存文件判断 gitalkCacheFile: path.join(__dirname, './gitalk-init-cache.json'), // 用于保存 gitalk 已经初始化的 id 列表 gitalkErrorFile: path.join(__dirname, './gitalk-init-error.json'), // 用于保存 gitalk 初始化报错的数据 }; const api = 'https://api.github.com/repos/' + config.username + '/' + config.repo + '/issues'; const sitemapXmlReader = (file) => { try { const data = fs.readFileSync(file, 'utf8'); const sitemap = xmlParser(data); let ret = []; sitemap.root.children.forEach(function (url) { const loc = url.children.find(function (item) { return item.name === 'loc'; }); if (!loc) { return false; } const title = url.children.find(function (item) { return item.name === 'title'; }); const desc = url.children.find(function (item) { return item.name === 'desc'; }); const date = url.children.find(function (item) { return item.name === 'date'; }); ret.push({ url: loc.content, title: title.content, desc: desc.content, date: date.content, }); }); return ret; } catch (e) { return []; } }; // 获取 gitalk 使用的 id const getGitalkId = ({ url: u, date }) => { const link = url.parse(u); // 链接不存在，不需要初始化 if (!link || !link.pathname) { return false; } if (!date) { return false; } return md5(link.pathname); }; /** * 通过以请求判断是否已经初始化 * @param {string} gitalk 初始化的id * @return {[boolean, boolean]} 第一个值表示是否出错，第二个值 false 表示没初始化， true 表示已经初始化 */ const getIsInitByRequest = (id) => { const options = { headers: { 'Authorization': 'token ' + config.token, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36', 'Accept': 'application/json' }, url: api + '?labels=' + id + ',Gitalk', method: 'GET' }; return new Promise((resolve) => { request(options, function (err, response, body) { if (err) { return resolve([err, false]); } if (response.statusCode != 200) { return resolve([response, false]); } const res = JSON.parse(body); if (res.length > 0) { return resolve([false, true]); } return resolve([false, false]); }); }); }; /** * 通过缓存判断是否已经初始化 * @param {string} gitalk 初始化的id * @return {boolean} false 表示没初始化， true 表示已经初始化 */ const getIsInitByCache = (() => { // 判断缓存文件是否存在 let gitalkCache = false; try { gitalkCache = require(config.gitalkCacheFile); } catch (e) { } return function (id) { if (!gitalkCache) { return false; } if (gitalkCache.find(({ id: itemId }) => (itemId === id))) { return true; } return false; }; })(); // 根据缓存，判断链接是否已经初始化 // 第一个值表示是否出错，第二个值 false 表示没初始化， true 表示已经初始化 const idIsInit = async (id) => { if (!config.cache) { return await getIsInitByRequest(id); } // 如果通过缓存查询到的数据是未初始化，则再通过请求判断是否已经初始化，防止多次初始化 if (getIsInitByCache(id) === false) { return await getIsInitByRequest(id); } return [false, true]; }; // 初始化 const gitalkInit = ({ url, id, title, desc }) => { //创建issue const reqBody = { 'title': title, 'labels': [id, 'Gitalk'], 'body': url + '\\r\\n\\r\\n' + desc }; const options = { headers: { 'Authorization': 'token ' + config.token, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36', 'Accept': 'application/json', 'Content-Type': 'application/json;charset=UTF-8' }, url: api, body: JSON.stringify(reqBody), method: 'POST' }; return new Promise((resolve) => { request(options, function (err, response, body) { if (err) { return resolve([err, false]); } if (response.statusCode != 201) { return resolve([response, false]); } return resolve([false, true]); }); }); }; /** * 写入内容 * @param {string} fileName 文件名 * @param {string} content 内容 */ const write = async (fileName, content, flag = 'w+') => { return new Promise((resolve) => { fs.open(fileName, flag, function (err, fd) { if (err) { resolve([err, false]); return; } fs.writeFile(fd, content, function (err) { if (err) { resolve([err, false]); return; } fs.close(fd, (err) => { if (err) { resolve([err, false]); return; } }); resolve([false, true]); }); }); }); }; const init = async () => { const urls = sitemapXmlReader(config.sitemap); // 报错的数据 const errorData = []; // 已经初始化的数据 const initializedData = []; // 成功初始化数据 const successData = []; for (const item of urls) { const { url, date, title, desc } = item; const id = getGitalkId({ url, date }); if (!id) { console.log(`id: 生成失败 [ ${id} ] `); errorData.push({ ...item, info: 'id 生成失败', }); continue; } const [err, res] = await idIsInit(id); if (err) { console.log(`Error: 查询评论异常 [ ${title} ] , 信息：`, err || '无'); errorData.push({ ...item, info: '查询评论异常', }); continue; } if (res === true) { // console.log(`--- Gitalk 已经初始化 --- [ ${title} ] `); initializedData.push({ id, url, title, }); continue; } console.log(`Gitalk 初始化开始... [ ${title} ] `); const [e, r] = await gitalkInit({ id, url, title, desc }); if (e || !r) { console.log(`Error: Gitalk 初始化异常 [ ${title} ] , 信息：`, e || '无'); errorData.push({ ...item, info: '初始化异常', }); continue; } successData.push({ id, url, title, }); console.log(`Gitalk 初始化成功! [ ${title} ] - ${id}`); continue; } console.log(''); // 空输出，用于换行 console.log('--------- 运行结果 ---------'); console.log(''); // 空输出，用于换行 if (errorData.length !== 0) { console.log(`报错数据： ${errorData.length} 条。参考文件 ${config.gitalkErrorFile}。`); await write(config.gitalkErrorFile, JSON.stringify(errorData, null, 2)); } console.log(`本次成功： ${successData.length} 条。`); // 写入缓存 if (config.cache) { console.log(`写入缓存： ${(initializedData.length + successData.length)} 条，已初始化 ${initializedData.length} 条，本次成功： ${successData.length} 条。参考文件 ${config.gitalkCacheFile}。`); await write(config.gitalkCacheFile, JSON.stringify(initializedData.concat(successData), null, 2)); } else { console.log(`已初始化： ${initializedData.length} 条。`); } }; init();"},{"title":"关于","date":"2021-04-28T13:38:44.833Z","updated":"2021-04-28T13:38:44.833Z","comments":false,"path":"about/index.html","permalink":"https://study4.fun/about/","excerpt":"","text":"北邮在读硕士研究生。北邮电子信息工程本科。"},{"title":"分类","date":"2021-04-28T13:38:44.833Z","updated":"2021-04-28T13:38:44.833Z","comments":false,"path":"categories/index.html","permalink":"https://study4.fun/categories/","excerpt":"","text":""},{"title":"友情链接","date":"2021-04-28T13:38:44.833Z","updated":"2021-04-28T13:38:44.833Z","comments":true,"path":"links/index.html","permalink":"https://study4.fun/links/","excerpt":"","text":""},{"title":"标签","date":"2021-04-28T13:38:44.833Z","updated":"2021-04-28T13:38:44.833Z","comments":false,"path":"tags/index.html","permalink":"https://study4.fun/tags/","excerpt":"","text":""}],"posts":[{"title":"Kubelet Image GC原理分析","slug":"Kubelet Image GC原理分析","date":"2021-04-25T07:48:00.000Z","updated":"2021-04-28T13:39:22.822Z","comments":true,"path":"2021-04-25-analysis-of-kubelet-imagegc/","link":"","permalink":"https://study4.fun/2021-04-25-analysis-of-kubelet-imagegc/","excerpt":"","text":"本文适用于想使用 Image GC 或想了解 Image GC，对 ImageGC 有二次开发需求的 K8S 用户。如果只是想使用配置，只需看前两段以及最后的总结即可。 Image GC 是什么？Image GC 是随着 kubelet 启动的镜像清理功能，用于在磁盘空间不足的情况下清除不需要的镜像，释放磁盘空间，保证 pod 正常启动运行。 Image GC 如何使用？kubelet 默认开启，通过 kubelet 内的 ImageGCPolicy 控制。ImageGCPolicy 有三个选项： ImageGCHighThresholdPercent：开始 gc 的阈值，超过该值将会触发 gc，设置为 100 时，gc 不启动。 ImageGCLowThresholdPercent：不会运行 gc 的下限值，gc 触发后，将会将磁盘占用率降至该值以下； ImageMinimumGCAge：最短 gc 时长，image age 小于该阈值时不会被 gc； 源码分析在 kubelet 启动流程中，image gc 的执行在 BirthCry 执行完成之后。 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) StartGarbageCollection() &#123; loggedContainerGCFailure := false // container gc流程，省略 ... // ImageGCHighThresholdPercent设置为100时，关闭image gc if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &#123; klog.V(2).Infof(\"ImageGCHighThresholdPercent is set 100, Disable image GC\") return &#125; prevImageGCFailed := false go wait.Until(func() &#123; if err := kl.imageManager.GarbageCollect(); err != nil &#123; if prevImageGCFailed &#123; klog.Errorf(\"Image garbage collection failed multiple times in a row: %v\", err) // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) &#125; else &#123; klog.Errorf(\"Image garbage collection failed once. Stats initialization may not have completed yet: %v\", err) &#125; prevImageGCFailed = true &#125; else &#123; var vLevel klog.Level = 4 if prevImageGCFailed &#123; vLevel = 1 prevImageGCFailed = false &#125; klog.V(vLevel).Infof(\"Image garbage collection succeeded\") &#125; &#125;, ImageGCPeriod, wait.NeverStop)&#125; 可以看到，image gc 由单独的协程执行，默认的 ImageGCPeriod 为五分钟，即每隔五分钟运行一次，不停止。当 image gc 重复失败时，会吐出 ImageGCFailed 的事件，首次失败仅会打印日志。这意味着可以通过配置日志或者告警了解 GC 是否正常运行。接下来看看 imageManager 的具体实现。 1234567891011121314151617181920212223242526272829303132333435363738type ImageGCManager interface &#123; // Applies the garbage collection policy. Errors include being unable to free // enough space as per the garbage collection policy. GarbageCollect() error // Start async garbage collection of images. Start() GetImageList() ([]container.Image, error) // Delete all unused images. DeleteUnusedImages() error&#125;func NewImageGCManager(runtime container.Runtime, statsProvider StatsProvider, recorder record.EventRecorder, nodeRef *v1.ObjectReference, policy ImageGCPolicy, sandboxImage string) (ImageGCManager, error) &#123; // Validate policy. if policy.HighThresholdPercent &lt; 0 || policy.HighThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf(\"invalid HighThresholdPercent %d, must be in range [0-100]\", policy.HighThresholdPercent) &#125; if policy.LowThresholdPercent &lt; 0 || policy.LowThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf(\"invalid LowThresholdPercent %d, must be in range [0-100]\", policy.LowThresholdPercent) &#125; if policy.LowThresholdPercent &gt; policy.HighThresholdPercent &#123; return nil, fmt.Errorf(\"LowThresholdPercent %d can not be higher than HighThresholdPercent %d\", policy.LowThresholdPercent, policy.HighThresholdPercent) &#125; im := &amp;realImageGCManager&#123; runtime: runtime, policy: policy, imageRecords: make(map[string]*imageRecord), statsProvider: statsProvider, recorder: recorder, nodeRef: nodeRef, initialized: false, sandboxImage: sandboxImage, &#125; return im, nil&#125; ImageGCManager 的接口非常简单，只有四个方法。初始化的时候就是校验 Policy 的参数，然后传递了几个相关的运行时、监控、事件等参数。然后看看启动： 1234567891011121314151617181920212223242526func (im *realImageGCManager) Start() &#123; go wait.Until(func() &#123; // Initial detection make detected time \"unknown\" in the past. var ts time.Time if im.initialized &#123; ts = time.Now() &#125; _, err := im.detectImages(ts) if err != nil &#123; klog.Warningf(\"[imageGCManager] Failed to monitor images: %v\", err) &#125; else &#123; im.initialized = true &#125; &#125;, 5*time.Minute, wait.NeverStop) // Start a goroutine periodically updates image cache. go wait.Until(func() &#123; images, err := im.runtime.ListImages() if err != nil &#123; klog.Warningf(\"[imageGCManager] Failed to update image list: %v\", err) &#125; else &#123; im.imageCache.set(images) &#125; &#125;, 30*time.Second, wait.NeverStop)&#125; ImageGCManager 的启动会启动两个协程。在第一个协程内，每隔五分钟 Manager 会通过检查镜像来确保初始化完成。另一个协程每隔 30 秒会将本机运行时上所有的镜像更新到缓存 imageCache 中。ImageCache 有什么作用以及镜像是如何检测的，可以放到后面 gc 的主流程里在仔细看。ImageGCManager 的核心方法就是 GarbageCollect 了，看源码步骤如下：首先获取 Image 对应的 Filesystem 占用信息，计算出用量百分比以及需要释放的空间大小，然后开始释放。如果实际释放的空间小于目标大小，会记录 FreeDiskSpaceFailed 的 Warnning 事件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (im *realImageGCManager) GarbageCollect() error &#123; // Get disk usage on disk holding images. fsStats, err := im.statsProvider.ImageFsStats() if err != nil &#123; return err &#125; var capacity, available int64 if fsStats.CapacityBytes != nil &#123; capacity = int64(*fsStats.CapacityBytes) &#125; if fsStats.AvailableBytes != nil &#123; available = int64(*fsStats.AvailableBytes) &#125; if available &gt; capacity &#123; klog.Warningf(\"available %d is larger than capacity %d\", available, capacity) available = capacity &#125; // Check valid capacity. if capacity == 0 &#123; err := goerrors.New(\"invalid capacity 0 on image filesystem\") im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.InvalidDiskCapacity, err.Error()) return err &#125; // If over the max threshold, free enough to place us at the lower threshold. usagePercent := 100 - int(available*100/capacity) if usagePercent &gt;= im.policy.HighThresholdPercent &#123; amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available klog.Infof(\"[imageGCManager]: Disk usage on image filesystem is at %d%% which is over the high threshold (%d%%). Trying to free %d bytes down to the low threshold (%d%%).\", usagePercent, im.policy.HighThresholdPercent, amountToFree, im.policy.LowThresholdPercent) freed, err := im.freeSpace(amountToFree, time.Now()) if err != nil &#123; return err &#125; if freed &lt; amountToFree &#123; err := fmt.Errorf(\"failed to garbage collect required amount of images. Wanted to free %d bytes, but freed %d bytes\", amountToFree, freed) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.FreeDiskSpaceFailed, err.Error()) return err &#125; &#125; return nil&#125; 在看 ImageGC 内磁盘释放空间的实现，需要了解 Manager 是如何检测镜像的，即 detectImages 方法。首先，sandboxImage 是一定会被判定为正在使用的镜像。接着会将所有 Pod 的所有 Container 使用的 image 加到正在使用的镜像列表中。然后，所有在本机器上已经存在的镜像会批量记录到 Manager 的探测记录中，同时将探测时间置为本轮探测的时间。Manager 的镜像探测记录中会记录该镜像的第一次被探测到的时间、最后一次使用的时间以及镜像大小。在将某个新 Image 添加到探测记录并记录时间的同时，无论是否是被第一次探测到，如果该镜像在前面的“正在使用的镜像”列表中，那么它的最新使用都会被刷新为当前的时间。最后，如果某个镜像已经不在本机上，就会被移出 Manager 的镜像探测记录。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func (im *realImageGCManager) detectImages(detectTime time.Time) (sets.String, error) &#123; imagesInUse := sets.NewString() // Always consider the container runtime pod sandbox image in use imageRef, err := im.runtime.GetImageRef(container.ImageSpec&#123;Image: im.sandboxImage&#125;) if err == nil &amp;&amp; imageRef != \"\" &#123; imagesInUse.Insert(imageRef) &#125; images, err := im.runtime.ListImages() if err != nil &#123; return imagesInUse, err &#125; pods, err := im.runtime.GetPods(true) if err != nil &#123; return imagesInUse, err &#125; // Make a set of images in use by containers. for _, pod := range pods &#123; for _, container := range pod.Containers &#123; klog.V(5).Infof(\"Pod %s/%s, container %s uses image %s(%s)\", pod.Namespace, pod.Name, container.Name, container.Image, container.ImageID) imagesInUse.Insert(container.ImageID) &#125; &#125; // Add new images and record those being used. now := time.Now() currentImages := sets.NewString() im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() for _, image := range images &#123; klog.V(5).Infof(\"Adding image ID %s to currentImages\", image.ID) currentImages.Insert(image.ID) // New image, set it as detected now. if _, ok := im.imageRecords[image.ID]; !ok &#123; klog.V(5).Infof(\"Image ID %s is new\", image.ID) im.imageRecords[image.ID] = &amp;imageRecord&#123; firstDetected: detectTime, &#125; &#125; // Set last used time to now if the image is being used. if isImageUsed(image.ID, imagesInUse) &#123; klog.V(5).Infof(\"Setting Image ID %s lastUsed to %v\", image.ID, now) im.imageRecords[image.ID].lastUsed = now &#125; klog.V(5).Infof(\"Image ID %s has size %d\", image.ID, image.Size) im.imageRecords[image.ID].size = image.Size &#125; // Remove old images from our records. for image := range im.imageRecords &#123; if !currentImages.Has(image) &#123; klog.V(5).Infof(\"Image ID %s is no longer present; removing from imageRecords\", image) delete(im.imageRecords, image) &#125; &#125; return imagesInUse, nil&#125; 刚才说到了镜像每次探测都会有记录，那么探测记录是用来干什么的？很容易理解，是给释放空间提供参考的。在完成镜像探测后，我们的得到的 imagesInUse 包括了 sanbox Image 以及 pod 内各 container 使用的镜像。接下来，要选出清理的目标镜像，这里存放的数据结构叫 evictionInfo，它主要存放了上面说到的每个的镜像探测记录。所有不在 imagesInUse 列表内的镜像都被列入到清理目标中。接着会将这些清理目标按照 最后使用时间 和 首次探测时间 进行一次排序，按照 LRU 规则将最早使用和最早探测的镜像排在前面。排序完之后，会遍历所有这些镜像：如果是本轮刚探测到的镜像不会删除。同时，该镜像距离第一次被探测到的时间差如果小于配置的最小 GC 间隔，也不会删除。否则，就会依序删除这些镜像，删除完之后会从探测记录中删除该镜像同时累加 已经释放的空间值 spaceFreed。如果 spaceFreed 不小于目标释放的空间，则本轮的清理结束。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (im *realImageGCManager) freeSpace(bytesToFree int64, freeTime time.Time) (int64, error) &#123; imagesInUse, err := im.detectImages(freeTime) if err != nil &#123; return 0, err &#125; im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() // Get all images in eviction order. images := make([]evictionInfo, 0, len(im.imageRecords)) for image, record := range im.imageRecords &#123; if isImageUsed(image, imagesInUse) &#123; klog.V(5).Infof(\"Image ID %s is being used\", image) continue &#125; images = append(images, evictionInfo&#123; id: image, imageRecord: *record, &#125;) &#125; sort.Sort(byLastUsedAndDetected(images)) // Delete unused images until we've freed up enough space. var deletionErrors []error spaceFreed := int64(0) for _, image := range images &#123; klog.V(5).Infof(\"Evaluating image ID %s for possible garbage collection\", image.id) // Images that are currently in used were given a newer lastUsed. if image.lastUsed.Equal(freeTime) || image.lastUsed.After(freeTime) &#123; klog.V(5).Infof(\"Image ID %s has lastUsed=%v which is &gt;= freeTime=%v, not eligible for garbage collection\", image.id, image.lastUsed, freeTime) continue &#125; // Avoid garbage collect the image if the image is not old enough. // In such a case, the image may have just been pulled down, and will be used by a container right away. if freeTime.Sub(image.firstDetected) &lt; im.policy.MinAge &#123; klog.V(5).Infof(\"Image ID %s has age %v which is less than the policy's minAge of %v, not eligible for garbage collection\", image.id, freeTime.Sub(image.firstDetected), im.policy.MinAge) continue &#125; // Remove image. Continue despite errors. klog.Infof(\"[imageGCManager]: Removing image %q to free %d bytes\", image.id, image.size) err := im.runtime.RemoveImage(container.ImageSpec&#123;Image: image.id&#125;) if err != nil &#123; deletionErrors = append(deletionErrors, err) continue &#125; delete(im.imageRecords, image.id) spaceFreed += image.size if spaceFreed &gt;= bytesToFree &#123; break &#125; &#125; if len(deletionErrors) &gt; 0 &#123; return spaceFreed, fmt.Errorf(\"wanted to free %d bytes, but freed %d bytes space with errors in image deletion: %v\", bytesToFree, spaceFreed, errors.NewAggregate(deletionErrors)) &#125; return spaceFreed, nil&#125; 总结总的来看，Kubelet 会在 Image 对应的 Filesystem 空间不足的情况下删除冗余的镜像。整个 GC 的要点如下： 清理的触发为到达 HighThresholdPercent 开始清理，一直清理到 LowThresholdPercent 为止。 在此过程中，有三类镜像不会被清除： sanbox 所需镜像； gc 首次探测到的镜像； 探测累计时长小于 MinimumGCAge 的镜像。 清理过程会优先清除最久没用到的和最早探测到的镜像。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Kubelet","slug":"Kubelet","permalink":"https://study4.fun/tags/Kubelet/"}]},{"title":"博客写作的工作流","slug":"博客写作的工作流","date":"2021-01-02T15:47:00.000Z","updated":"2021-04-28T13:39:22.986Z","comments":true,"path":"2021-01-02-blogs-workflow/","link":"","permalink":"https://study4.fun/2021-01-02-blogs-workflow/","excerpt":"","text":"引言一开始就把博客写作叫成一个工作流，是因为我们很多时候在写博客的时候，懒惰的很大一部分原因是因为觉得麻烦。当你在使用 hexo 这种静态博客的时候，难免会觉得写作是一件很繁琐的时间，首先会受限于写作环境，其次每次写作都需要“写 md –&gt; 插入图片上传 –&gt; 预览后编译 –&gt; 推送到 pages”。因为工作的原因，用了很久的语雀，感觉语雀的写作体验和管理都是很棒的。那么，如何拥有语雀写作的体验，又能够免去博客更新的繁碎流程呢？ 关于工作流实现配置的文章网上一堆，我就不当个搬运工了，说下整个流程吧： 在语雀的指定仓库写文章，勾选发布的“文档有较大更新，推送给关注知识库的人” 你在该仓库下设置的 WebHook 触发云平台上的函数服务 函数服务发送对应的请求给 Github 的 repo dispatch dispatch 触发 Github Action，开始构建网页并推送到目标平台（Pages/COS/OSS)，推送完毕后自动刷新 CDN 完成后，获得的是这样的一套写作平台体验： 复用了语雀优秀的文档写作和管理体验，做到 Write Every Where 除了域名外，几乎全免费 省去了博客的维护成本（Serverless？） 当然了，初次配置的成本也是不低的。不过对于程序员来说，哪怕首次配置麻烦，后面能省掉很多时间成本也是一件很赚的事情吧。（何况语雀比本地 markdown 好用多了）而且每次写作不用限制在某一台电脑上。 一些细节 语雀的 TOKEN 权限设置为只读就 OK 可以使用语雀作为图床白嫖一下，需要额外配置一下语雀的防盗链 国内的收录需要博客在国内可访问，使用 Pages+CDN 可以达到效果，又拍云免费 https 证书操作比较麻烦，可以用第三方网站签发免费的泛域名证书，只是记得定时要在 CDN 上去配置 刷新 CDN 的 API 调用记得配置全站路径 参考语雀知识库同步工具Hexo：语雀云端写作，Github Actions 持续集成","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"}]},{"title":"CGroup初探","slug":"CGroup初探","date":"2020-10-30T12:55:00.000Z","updated":"2021-04-28T13:39:22.906Z","comments":true,"path":"2020-10-30-cgroup-startings/","link":"","permalink":"https://study4.fun/2020-10-30-cgroup-startings/","excerpt":"","text":"基本概念namespace：隔离进程组之间的资源cgroup：对一组进程进行统一的资源监控和限制，进程按组进行管理的机制cgroup 的构成： subsystem：对应内核模块，用于对进程组进行操作。 hierarchy：一棵 cgroup 树，用于进程分组。 其他： 在任意一个 hierarchy 内包含 linux 系统的所有 process。 在同一个 hierarchy 内，process 属于唯一的 process group。但 process 可以存在于不同的 hierarchy 中。 hierarchy 会对应若干不同的 subsystem（可以为 0），一个 subsystem 只能关联一个 hierarchy。systemd 没有对应的 subsystem。 process -&gt; process group（cgroup） -&gt; hierarchy树节点 -&gt; 树 Subsystemsubsystem namesubsystem 关联 hierarchy id：如果为 0，无绑定/cgroup v2 绑定/未被内核开启num of cgroups：关联 hierarchy 内进程组个数enabled：是否开启，通过内核参数 cgroup_disable 调整 详细cpu (since Linux 2.6.24; CONFIG_CGROUP_SCHED)用来限制 cgroup 的 CPU 使用率。 cpuacct (since Linux 2.6.24; CONFIG_CGROUP_CPUACCT)统计 cgroup 的 CPU 的使用率。 cpuset (since Linux 2.6.24; CONFIG_CPUSETS)绑定 cgroup 到指定 CPUs 和 NUMA 节点。 memory (since Linux 2.6.25; CONFIG_MEMCG)统计和限制 cgroup 的内存的使用率，包括 process memory, kernel memory, 和 swap。 devices (since Linux 2.6.26; CONFIG_CGROUP_DEVICE)限制 cgroup 创建(mknod)和访问设备的权限。 freezer (since Linux 2.6.28; CONFIG_CGROUP_FREEZER)suspend 和 restore 一个 cgroup 中的所有进程。 net_cls (since Linux 2.6.29; CONFIG_CGROUP_NET_CLASSID)将一个 cgroup 中进程创建的所有网络包加上一个 classid 标记，用于 tc 和 iptables。 只对发出去的网络包生效，对收到的网络包不起作用。 blkio (since Linux 2.6.33; CONFIG_BLK_CGROUP)限制 cgroup 访问块设备的 IO 速度。 perf_event (since Linux 2.6.39; CONFIG_CGROUP_PERF)对 cgroup 进行性能监控 net_prio (since Linux 3.3; CONFIG_CGROUP_NET_PRIO)针对每个网络接口设置 cgroup 的访问优先级。 hugetlb (since Linux 3.5; CONFIG_CGROUP_HUGETLB)限制 cgroup 的 huge pages 的使用量。 pids (since Linux 4.3; CONFIG_CGROUP_PIDS)限制一个 cgroup 及其子孙 cgroup 中的总进程数。 使用12345678910# 挂载一颗和cpuset subsystem关联的cgroup树到&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpusetmkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpusetmount -t cgroup -o cpuset xxx &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuset# 不关联任何子系统mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemdmount -t cgroup -o none,name&#x3D;systemd xxx &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemd# 关联所有子系统mount -t cgroup xxx &#x2F;sys&#x2F;fs&#x2F;cgroup 查看进程对应的 cgroupproc/[pid]/cgrouphierarchy id : subsystems : 进程在 hierarchy 中的相对路径 创建管理 cgroupcgroup 目录 cgroup.clone_children这个文件只对 cpuset subsystem 有影响，当该文件的内容为 1 时，新创建的 cgroup 将会继承父 cgroup 的配置，即从父 cgroup 里面拷贝配置文件来初始化新 cgroup，可以参考这里 cgroup.procs当前 cgroup 中的所有进程 ID，系统不保证 ID 是顺序排列的，且 ID 有可能重复 cgroup.sane_behavior具体功能不详，可以参考这里和这里 notify_on_release该文件的内容为 1 时，当 cgroup 退出时（不再包含任何进程和子 cgroup），将调用 release_agent 里面配置的命令。新 cgroup 被创建时将默认继承父 cgroup 的这项配置。 release_agent里面包含了 cgroup 退出（移出）时将会执行的命令，系统调用该命令时会将相应 cgroup 的相对路径当作参数传进去。 注意：这个文件只会存在于 root cgroup 下面，其他 cgroup 里面不会有这个文件。 tasks当前 cgroup 中的所有线程 ID，系统不保证 ID 是顺序排列的 创建子 cgroup新建子文件夹 添加进程 在一颗 cgroup 树里面，一个进程必须要属于一个 cgroup。 新创建的子进程将会自动加入父进程所在的 cgroup 从一个 cgroup 移动一个进程到另一个 cgroup 时，只要有目的 cgroup 的写入权限就可以了，系统不会检查源 cgroup 里的权限。 用户只能操作属于自己的进程，不能操作其他用户的进程，root 账号除外 打印当前 shell pidecho $$ 1sh -c &#39;echo 1421 &gt; ..&#x2F;cgroup.procs&#39; 将 pid 写入，即是进程加入 cgroup。pid 不可在文件中删除，只可以被转移。因为 在一颗 cgroup 树里面，一个进程必须要属于一个 cgroup Pid Subsystempids.current: 表示当前 cgroup 及其所有子孙 cgroup 中现有的总的进程数量pids.max: 当前 cgroup 及其所有子孙 cgroup 中所允许创建的总的最大进程数量通过写入 pids.max 限制成功 子孙 cgroup 中的 pids.max 大小不能超过父 cgroup 中的大小 子 cgroup 中的进程数不仅受自己的 pids.max 的限制，还受祖先 cgroup 的限制 pids.current &gt; pids.max 出现的情况： 设置 pids.max 时，将其值设置的已经比 pids.current 小 pids.max 只会在当前 cgroup 中的进程 fork、clone 的时候生效，将其他进程加入到当前 cgroup 时，不会检测 pids.max Memory Subsystem 1234567891011121314cgroup.event_control #用于eventfd的接口 memory.usage_in_bytes #显示当前已用的内存 memory.limit_in_bytes #设置&#x2F;显示当前限制的内存额度 memory.failcnt #显示内存使用量达到限制值的次数 memory.max_usage_in_bytes #历史内存最大使用量 memory.soft_limit_in_bytes #设置&#x2F;显示当前限制的内存软额度 memory.stat #显示当前cgroup的内存使用情况 memory.use_hierarchy #设置&#x2F;显示是否将子cgroup的内存使用情况统计到当前cgroup里面 memory.force_empty #触发系统立即尽可能的回收当前cgroup中可以回收的内存 memory.pressure_level #设置内存压力的通知事件，配合cgroup.event_control一起使用 memory.swappiness #设置和显示当前的swappiness memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去 memory.oom_control #设置&#x2F;显示oom controls相关的配置 memory.numa_stat #显示numa相关的内存 设置了内存限制立即生效 –&gt; 物理内存使用量达到 limit –&gt; memory.failcnt +1 –&gt; 内核会尽量将物理内存中的数据移到 swap 空间上去 –&gt; 设置的 limit 过小，或者 swap 空间不足 –&gt; kill 掉 cgroup 内继续申请内存的进程（默认行为）详细链接 ## CPU Subsystemcfs_period_us：时间周期长度cfs_quota_us：在一个周期长度内所能使用的 CPU 时间数cpu.shares： 针对所有的 CPU 的相对值，默认值是 1024 仅在 CPU 忙时起作用 无法精确的控制 CPU 使用率，因为 cgroup 会动态变化 limit_in_cores = cfs_period_us / cfs_quota_us Refhttps://segmentfault.com/a/1190000006917884","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"cgroup","slug":"cgroup","permalink":"https://study4.fun/tags/cgroup/"}]},{"title":"QoS in Kubernetes","slug":"QoS in Kubernetes","date":"2020-10-26T05:55:00.000Z","updated":"2021-04-28T13:39:22.954Z","comments":true,"path":"2020-10-26-qos-in-kubernetes/","link":"","permalink":"https://study4.fun/2020-10-26-qos-in-kubernetes/","excerpt":"","text":"1.原则 通过 limit 保证不同 pod 只能占用指定的资源 集群资源允许被超额分配 为 pod 划分等级确保不同的 QoS，资源不足时，低等级的会被清理 2.语义limits系统允许容器运行时可能使用的资源量的最高上限最多允许使用的上限，超过时进程会被杀掉 requestsK8S 调度时能为容器提供的完全可保障的资源量最少的资源下限，当 node 上资源少于该值，pod 将不会被调度到此 node 上 m=milli unit，表示千分之一M=1000 3.资源范围cpu/mem/gpu/huge-page(v1.14) 123resources: limits: hugepages-2Mi: 100Mi 4.基于 request 和 limit 的调度机制调度时不看实际的使用资源量，看已运行 pod 的 request 总和作为已占用资源的度量 K8S pod 资源的特点分为 完全可靠资源 和 不可靠资源，通过这种机制实现 超卖完全可靠的资源 = request不可靠资源 = limit - request 可压缩/不可压缩资源可压缩资源：CPU空闲资源按照 Request 的比例进行分配pod 的 cpu 使用超过 limit 时，cgroups 会对 pod 进行限流 throttled 不可压缩资源：内存超过 request 可能会被杀掉超过 limit 时，内核会杀掉容器中使用内存最多的一个，直到不超过 limnit 为止 5. 服务质量等级 QoS Class优先级递减：Guaranteed &gt; Burstable &gt; BestEffort Guaranteed所有容器 request=limit（仅设置 Limit 时也等效） Burstablerequests 不等于 limits BestEffort所有容器的 request 和 limit 都未定义 OOM ScoreOOM Score = 内存占用百分比 * 10 + 调整分（OOM_SCORE_ADJ) OOM_SCORE_ADJGuaranteed: -998, BestEffort: 1000,Busrtable: request &gt; 99.8%内存, 2 &lt;，1000 - 内存占用百分比 * 10 request=0, 999 特殊的 OOM_SCOREkubelet/docker: -998不会被杀掉的进程： -999","categories":[{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"APUE读书笔记（一）","slug":"APUE读书笔记（一）","date":"2020-08-15T15:55:00.000Z","updated":"2021-04-28T13:39:23.034Z","comments":true,"path":"2020-08-15-apue-readling-notes-1/","link":"","permalink":"https://study4.fun/2020-08-15-apue-readling-notes-1/","excerpt":"","text":"一. 基础知识1.1 体系结构系统调用：内核的接口应用程序既可以使用共用函数库，也可以使用系统调用 1.2 登录登录项的组成：登录名、加密口令、数字用户 ID（UID）、数字组 ID（GID）、注释字段、起始目录、shell 程序 1.3 目录每个进程都有一个工作目录（当前工作目录），相对路径都从工作目录开始解析进程可以使用 chdir 函数更改工作目录 1.4 输入输出1.4.1 文件描述符文件描述符是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表取值范围为 0 到 OPEN_MAX（每个进程最多可以打开 的文件数-1，63） 1.4.2 标准输入、标准输出和标准错误标准输入、标准输出和标准错误是在运行新程序时，shell 为程序默认打开的 3 个文件描述符 1.5 程序和进程内核使用 exec 函数将程序读入内存并执行程序fork 函数创建一个新进程，该进程（子进程）是调用进程（父进程）的一个副本在创建时，对父进程返回子进程 ID，对子进程返回 0 1.6 线程一个进程内的所有线程共享同一地址空间、文件描述符、栈以及与进程相关的属性 1.7 出错处理errno：Unix 系统函数出错时返回的负值关于 errno： 如果没有出错，其值都不会被例程清除 任何函数都不会将 errno 设置为 0 关于错误： 出错分为致命性的和非致命性的 对于致命性错误，可打印出错消息或者写入日志 对于非致命错误，可以尝试重试（延时、指数补偿算法） 1.8 用户标识1. 用户 ID用户不可更改其用户 ID用户 ID 为 0 的用户为 root 用户或超级用户 2. 组 ID组文件将组名映射为数字的组 ID，位于/etc/group 3. 附属组 ID系统允许用户属于另外一些组，一个用户属于多至 16 个其他的组 1.9 信号信号用于通知进程发生了某种状况处理信号的方式： 忽略 系统默认方式处理 提供函数，信号发生时调用，即捕捉信号 1.10 时间日历时间：自 UTC 时间的秒数累计值进程时间：也叫做 CPU 时间，用来度量进程使用的 CPU 资源，以时钟 tick 计算系统为进程维护的三个进程时间值： 时钟时间，进程运行的时间总量（与系统同时运行的进程数有关） 用户 CPU 时间，执行用户指令所用的时间量 系统 CPU 时间，为该进程执行内核程序经历的时间量 用户 CPU 时间和系统 CPU 时间常称为 CPU 时间 12# 度量执行时间time -p &lt;command&gt; 1.11 系统调用和库函数系统调用：提供给程序向内核请求服务的，操作系统提供的入口点区别： 库函数可以被替换，而系统调用通常不可替换 应用程序既可以调用 系统调用，也可以调用 库函数 许多库函数会调用系统调用 系统调用通常是提供最接口，而库函数通常提供较复杂的功能 进程控制系统调用（fork/exec/wait）通常有用户应用程序直接调用","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Unix","slug":"Unix","permalink":"https://study4.fun/tags/Unix/"}]},{"title":"《代码整洁之道》笔记","slug":"《代码整洁之道》笔记","date":"2020-05-25T02:08:00.000Z","updated":"2021-04-28T13:39:23.158Z","comments":true,"path":"2020-05-25-clean-coder-notes/","link":"","permalink":"https://study4.fun/2020-05-25-clean-coder-notes/","excerpt":"","text":"1. 专业主义1.1 担当责任1.2 不行损害之事（1）不要破坏软件功能，让失误率无限接近于 0 让测试人员找不出问题 不发布无把握的代码 反思 bug 怎么越过测试 确信代码正常运行（自动化测试、单元测试） 自动化 QA （2）不要破坏结构 不为了发布新功能破坏代码的结构 软件要易于修改（需要使用测试保证代码可经常修改） 1.3 职业道德职业发展需要保证每周有自己的时间 领域知识（设计模式、设计原则、方法、实践、工件） 坚持学习（文章、博客、技术大会） 练习（每天一到两道题） 合作 辅导 了解业务领域（了解基础架构、基本知识、原则和价值观念） 站在用户的角度思考 谦逊 2. 说“不” 能就是能，不能就是不能。不要说“试试看”。 作为专业人士，就不应该什么事都照做 2.1 对抗角色面对艰难的决定，直面不同角色的冲突是最好的办法——找到可能的最好结果 “为什么”其实并没有那么重要 2.2 高风险时刻在高风险时刻说不，是对大家负责 2.3 团队精神 不说谎，做出合理预期 如果是“试试看”，那就代表之前留有余力。本质上是在说谎，可能是为了护住面子和避免冲突 不消极对抗，不能任由事态发展 2.4 如何写出好代码坚守专业主义精神，说不 3. 说“是”3.1 承诺用语的三个阶段 口头上说。缺乏承诺，通常是“需要、但愿、让我们”等 心里认真。有清晰的事实陈述，明确说明了期限 真正付诸行动。 3.2 如何做到“言必信，行必果” 如果目标依赖于他人，就应该采取具体行动，推动最终目标 即使感觉目标无法完成，也要弄清楚目标是否能够达成 如果做不到，就应尽早向承诺对象发起预警 3.3 如何说“是”不说“试试”，直接说出所有的可能性 坚守原则 4. 编码4.1 编码需要考虑的问题 代码正常工作 解决客户提出的问题——不是需求，需求不一定能解决问题 和现有的系统能很好的结合 其他程序员可以读懂你的代码 不要在疲劳或者焦虑的时候编码 4.2 流态区这种状态下，感觉效率极高、绝无错误 避免进入流态区，因为这样会进入“放弃顾及全局”的陷阱 4.3 思路阻塞 找个搭档结对编程 创造性输入（PPT、音乐、电影等） 4.4 调试调试和编码一样重要 TDD 可以减少调试时间 4.5 保持节奏在疲劳的时候应该离开工作 回家路上把自己从工作中抽离出来 洗澡时或许会浮现解决方案 4.6 关于进度延迟 不盲目期望提前完成 不盲目冲刺（可以考虑所有情况，缩减交付的范围） 加班（需要保证有个人有时间、短期加班、有加班失败的后备预案） 交付失误时不自欺欺人 定义一个确切的“完成”标准（自动化验收测试） 4.7 帮助他人 帮助他人的时候不应付 学会接受他人的帮助，学会请求帮助 辅导与寻求辅导 5. 测试驱动开发三项法则 在写好失败单元测试前，不要写任何产品代码； 一旦单元测试失败了，就不要写测试代码； 产品代码只需要恰好能够让当前失败的单元测试通过即可，不要多写。 优势 确定性；对代码的掌控程度高。 缺陷注入率；降低代码的 bug 和风险。 勇气；测试完备，修改或者重构的风险降低。 文档；单元测试即示例，清晰准确。 设计；迫使自己考虑好的设计。 6. 练习6.1 练习方式kata练习解决这个问题所需要的动作和决策 学习热键和导航操作、测试驱动开的、持续集成之类的方法 wasa两个人的 kata，一个人写单元测试，另一个人写程序 自由练习多人参与的 wasa 6.2 拓展经验的方法开源 使用自己的时间练习 7.验收测试7.1 需求的沟通不要过早精细化需求 不确定原则（观察者效应）：观察到的新信息会影响对整个系统的看法 预估焦虑：需求一定会变化，追求早期的精准性是徒劳的 迟来的模糊性：需求文档中的模糊之处，都对应着业务方的分歧 7.2 验收测试“完成”的含义 所有的代码写完了 测试通过了 QA 和需求方已经认可 沟通：开发方、业务方、测试方达成共识 自动化：缩减成本 测试并不是额外工作 验收测试与开发应当不是同一个人编写 开发人员的角色是把验收测试和开发系统联系起来保证测试的通过 不能被动接收测试，需要协商并改进 验收测试和单元测试：内部 vs 外部 图形界面和其他复杂因素：调用 API 而不是 GUI 持续集成：失败应立刻终止 8. 测试策略自动化测试金字塔： 单元测试：在最低层次上定义系统。 组件测试：对业务规则的验收测试。 集成测试：装备测试，确认组件之间正确连接，彼此通信畅通。 系统测试：测试系统是否正确组装完毕，以及各个组件之间是否正确交互，同时包括吞吐量测试和性能测试。 人工探索式测试：确保系统在人工才操作表现良好。 9. 时间管理9.1 会议学会拒绝/离席，确定议程和目标，立会，迭代计划会议，争论与反对 凡是不能在五分钟内解决的争论，都不能靠辩论解决。唯一的出路是：用数据说话。可以做试验、模仿或者建模。 9.2 注意力点数学会安排时间，妥善使用自己的注意力点数 睡眠/咖啡/恢复/肌肉注意力/输入和输出 9.3 时间拆分和番茄工作法记录下来并画图展示 9.4 要避免的行为优先级错乱——提高某个任务的优先级，有借口推迟真正急迫的任务 9.5 死胡同不执拗于不容放弃也无法绕开的主意，听取其他意见。越是坚持，浪费的时间越多。 9.6 泥潭走回头路是最简单的办法。 发现身处泥潭还固执前进，是最严重的优先级错乱。 10. 预估10.1 什么是预估承诺还是猜测？ 预估是一种猜测，不包含任何承诺的色彩。不是一个定数，是一种概率分布。 小心给出暗示性的承诺。 10.2 PERT（计划评审技术）乐观预估（1%），标称预估（概率最大），悲观预估（1%） 得出任务的期望完成时间和任务的概率分布标准差（不确定性） 10.3 预估任务德尔菲法： 亮手指：预估相近达到统一，有分歧则讨论 规划扑克 关联预估：多人对任务进行所需时间长短的排序，讨论分歧，任务归类 三元预估：使用德尔菲法分别进行乐观、标称、悲观预估 10.4 大数定律控制错误的方法——把大任务分成小任务，分开预估在加总，结果会比单独评估大任务要准确的多 11. 压力11.1 避免压力避免对没有把握能够达成的最后期限做出承诺 让系统、代码和设计尽可能整洁 在危机时也要遵守纪律原则（例如 TDD） 11.2 应对压力不要惊慌失措：深思熟虑，努力寻找可以带来最好结果的方法 沟通：告诉你制定的走出困境的计划，请求支援和指引 坚信纪律和原则 寻求帮助：结对编程 12. 协作首要职责是满足雇主的需求，和团队协作，深刻理解业务目标，了解企业如何从你的工作中获得回报 拥有代码的是整个团队，而不是个人 专业人士会共同工作，彼此面对面。 13. 团队和项目形成团队需要时间。建立关系，学会互相协作，了解彼此的癖好和长短处，最后凝聚成为团队。 把项目分配给已经形成凝聚力的团队，而不是围绕项目组建团队。 14. 辅导、实习生和技艺辅导： 通过书籍手册向作者学习 通过观察他人工作来学习 实习生： 技术方面你的传授、培训、督导和检查 技艺： 技艺的模因包含价值观、原则、技术、态度和正见 技艺的模因经由口口相传和手手相承而来","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"2019的闲言碎语","slug":"2019的闲言碎语","date":"2020-01-01T08:45:58.000Z","updated":"2021-04-28T13:39:23.546Z","comments":true,"path":"2020-01-01-2019-comments/","link":"","permalink":"https://study4.fun/2020-01-01-2019-comments/","excerpt":"","text":"回顾总是有种从时间线开始的惯性。2019 年开始于天津不好吃的本地餐厅，结束于一顿火烧云不好吃的外卖。如同不好吃的饭，其实这一年经历的过程也算不上顺遂心意。 平行的工作线年前的两个月忙着做实验室的项目，年后开始准备实习的事情。我常说自己足够幸运，在应该做某件事的时候就去做了这件事，这样尽力就好。但这也只是在事后对于自己的些微自嘲而已，我还是改不掉过分焦虑的坏毛病。正如高考前失眠，考研前默背政治到半夜，准备实习的我还是经常会自己一个人默默焦虑地胡思乱想到失眠。 中间经历了其实不算多的面试，因为想要投的岗位对口的其实并不算多。在实验室的经历让我走了一条和别的同学不太一样的岗位，得益于平时的兴趣比较杂比较宽泛，算是很幸运的拿到了实习岗位。其实回顾来看，最后的结果其实是超乎于我自己意料之外的，因为入职后见过身边远远优秀与我的同学太多。 如果对当前的选择感到迷茫，不如找一个能看清方向的人交流，远比自己尝试碰壁或者和同类人无效讨论要有效得多。希望自己能在以后的经历中谨记住这一点。 工作了三四个月，回顾下其实觉得收益到最多的反而是工作方式。很多时候，我们抱怨工作枯燥，内容无趣。但总是会有人能从这些枯燥的工作中提取出更有效的东西，提高自己的效率，提高自己的思考深度，这让我真切体会到平凡和优秀的区别。说和做，差的总是很远。 后面回到学校准备毕设中期，除了完成一篇自己之前觉得有点难度的水论文，倒也没有什么大的波澜。 平行的生活线对于个人生活来说，19 年真的是很重要的一年。不同于工作上能够絮絮叨叨说出个所以然，对于我来说，生活是由一个个或难忘或开心或感动，也或是平凡幸福的时刻组成的。很多个这样的时刻会慢慢沉淀成一种潜移默化的东西，支撑自己在各种困难的时候坚持过去。今年这样的时刻格外的多，或许因为不再是一个人的原因吧。 如果说要给自己的 2020 寄托一些什么希冀一些什么，大概还是一些意识到却没有做到的东西。 能够想到就去做，但是还没坚持做下去。 能够开始反思，但是还没及时改变。 其实如果能做到这两点，总感觉 2020 就会好。但是人总是有点惰性的，我也还是那个对自己有限悲观的人。接下来的一年，能做到一个，也足够让我满足了。毕竟，顺遂心意永远是最重要的。拧巴地生活，还不如维持现状呢。","categories":[{"name":"日记","slug":"日记","permalink":"https://study4.fun/categories/%E6%97%A5%E8%AE%B0/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"}]},{"title":"Kubernetes学习笔记——DesiredStateOfWorldPopulator源码分析","slug":"Kubernetes学习笔记——DesiredStateOfWorldPopulator源码分析","date":"2019-09-03T11:20:00.000Z","updated":"2021-04-28T13:39:23.210Z","comments":true,"path":"2019-09-03-desired-state-of-world-populator/","link":"","permalink":"https://study4.fun/2019-09-03-desired-state-of-world-populator/","excerpt":"","text":"我们知道，kubelet 启动时会运行 VolumeManager 协程来负责 Volume 变更时的操作。它主要通过 ActualStateOfWorld 和 DesiredStateOfWorld 这两个 cache 信息来让 VolumeManager 中的两个协程工作。 DesiredStateOfWorldPopulator 和 Reconciler 两个 Goroutine 会通过图中两个的 StateOfWorld 状态进行通信，DesiredStateOfWorldPopulator 主要负责从 Kubernetes 节点中获取新的 Pod 对象并更新 DesiredStateOfWorld 结构；而后者会根据实际状态和当前状态的区别对当前节点的状态进行迁移，也就是通过 DesiredStateOfWorld 中状态的变更更新 ActualStateOfWorld 中的内容。Ref: https://draveness.me/kubernetes-volume 下面就来分析一下 DesiredStateOfWorldPopulator 的源码。 0x02. 结构与接口DesiredStateOfWorldPopulator 的数据结构如下： 123456789101112131415type desiredStateOfWorldPopulator struct &#123; kubeClient clientset.Interface loopSleepDuration time.Duration getPodStatusRetryDuration time.Duration podManager pod.Manager podStatusProvider status.PodStatusProvider desiredStateOfWorld cache.DesiredStateOfWorld actualStateOfWorld cache.ActualStateOfWorld pods processedPods kubeContainerRuntime kubecontainer.Runtime timeOfLastGetPodStatus time.Time keepTerminatedPodVolumes bool hasAddedPods bool hasAddedPodsLock sync.RWMutex&#125; kubeClient：用以从 API Server 获取 PV 和 PVC 对象 loopSleepDuration：定义连续执行的间隔 podManager：host 真实存在的 Pod 信息获取来源 DesiredStateOfWorldPopulator 的接口有三个方法： 1234567type DesiredStateOfWorldPopulator interface &#123; Run(sourcesReady config.SourcesReady, stopCh &lt;-chan struct&#123;&#125;) ReprocessPod(podName volumetypes.UniquePodName) HasAddedPods() bool&#125; 除了核心执行方法 Run，ReprocessPod 能够将特定 Pod 强制剔出 processedPods 列表进行强制重新处理。该方法用于在 Pod 更新上启用重新挂载卷。而 HasAddedPods 方法则返回 populator 是否已经将所有现有 Pod 处理添加到 desired state 中。 0x03. 核心流程分析3.1 populatorLoopFuncrun 方法中，每隔 loopSleepDuration 就会执行一次 populatorLoopFunc。 1234567891011121314151617func (dswp *desiredStateOfWorldPopulator) populatorLoopFunc() func() &#123; return func() &#123; dswp.findAndAddNewPods() if time.Since(dswp.timeOfLastGetPodStatus) &lt; dswp.getPodStatusRetryDuration &#123; glog.V(5).Infof( \"Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).\", dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration), dswp.getPodStatusRetryDuration) return &#125; dswp.findAndRemoveDeletedPods() &#125;&#125; 3.2 findAndAddNewPodsfindAndAddNewPods 遍历所有 Pod 并且将“应该添加到期望状态但实际上没有添加”的 Pod 添加到对应状态值中。 分析流程可知该方法先寻找不是终止状态的 Pod，再调用 processPodVolumes 处理这些符合条件的 Pod。 123456789func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() &#123; for _, pod := range dswp.podManager.GetPods() &#123; if dswp.isPodTerminated(pod) &#123; // Do not (re)add volumes for terminated pods continue &#125; dswp.processPodVolumes(pod) &#125;&#125; 终止状态判定生效条件满足一条即判定为终止状态： Phase 处于 PodFailed Phase 处于 Succeeded 删除时间不为空，且所有内部容器状态 ContainerStatus 都为 Terminated 或者 Waiting，或者 Container List 为空。 终止状态判定完毕，核心方法 processPodVolumes 会将给定 Pod 中的 Volumes 进行处理并添加到期望状态值中。 1234567891011121314151617181920212223242526272829303132// processPodVolumes processes the volumes in the given pod and adds them to the desired state of the world.func (dswp *desiredStateOfWorldPopulator) processPodVolumes(pod *v1.Pod) &#123; ... uniquePodName := util.GetUniquePodName(pod) if dswp.podPreviouslyProcessed(uniquePodName) &#123; return &#125; allVolumesAdded := true mountsMap, devicesMap := dswp.makeVolumeMap(pod.Spec.Containers) // Process volume spec for each volume defined in pod for _, podVolume := range pod.Spec.Volumes &#123; volumeSpec, volumeGidValue, err := dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mountsMap, devicesMap) .... // Add volume to desired state of world _, err = dswp.desiredStateOfWorld.AddPodToVolume( uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue) .... &#125; if allVolumesAdded &#123; dswp.markPodProcessed(uniquePodName) // New pod has been synced. Re-mount all volumes that need it dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName) &#125;&#125; 可以看到，该方法处理流程如下： 判断该 Pod 之前是否被处理过，处理过则返回。即从 dswp 的 processedPods 列表中查找，需要加读锁。 通过 Container 列表创建 mountsMap 和 devicesMap，mountsMap 存储 VolumeMounts 字段的挂载信息，devicesMap 存储 BlockVolume 的信息。Key=VolumeMount.Name，Value=True。 对 Spec 中的 Volumes 列表中每一项 PodVolume，根据 PodName, Namespace, mountsMap, devicesMap 创建 VolumeSpec。 根据 Pod,PodName,VolumeSpec， PV Name，GID 将 Volume 添加到 dswp 缓存的 desiredStateOfWorld 中。 如果全部 Volume 的添加都成功则将 Pod 标记为“Processed”，同时将 Pod 标记为 RemountRequired 状态用以更新 Volume 的内容。 其中，步骤 3 的 createVolumeSpec 首先会判断该 podVolume 的 Source 是否为 PVC，如果为 PVC 则需要找到 Claim 背后的 PV Name，再通过 PV Name 获取真正的 PV 对象并返回。如果 PVC 为空，则对 PV 深拷贝并创建 Spec 对象返回。 1234567891011121314151617func (dswp *desiredStateOfWorldPopulator) getPVCExtractPV( namespace string, claimName string) (string, types.UID, error) &#123; pvc, err := dswp.kubeClient.CoreV1().PersistentVolumeClaims(namespace).Get(claimName, metav1.GetOptions&#123;&#125;) if err != nil || pvc == nil &#123; return \"\", \"\", fmt.Errorf(......) &#125; ... if pvc.Status.Phase != v1.ClaimBound || pvc.Spec.VolumeName == \"\" &#123; return \"\", \"\", fmt.Errorf(......) &#125; return pvc.Spec.VolumeName, pvc.UID, nil&#125; 实际上，通过 PVC 找 PV Name 是由 KubeClient 向 API Server 请求得到的。请求通过 Namespace 和 Claim Name 获取到 PVC 对象，确认 PVC 对象的 Phase 为 Bound 状态且 pvc.Spec.VolumeName 不为空。上述流程成功后返回 PVC 的 VolumeName（即 PV Name）和该 PVC 的 UID。 获取到 pvName 和 pvcUID 后，再次通过 KubeClient 向 API Server 请求得到 PV 对象。请求成功后检查 ClaimRef 是否为空，ClaimRef 的 UID 和传入的 PVC UID 是否一致。最后返回该 PV 对象，在返回的同时一并返回的还有 PV 的 GID。 再看看步骤 4，其调用的 AddPodToVolume 方法如果检查到没有可用的 Volume 插件或者可用插件不止一个，会返回 Error。如果 Pod Unique Name 重复，则不执行任何操作。此外，如果 Volume Name 如果不在该节点的 Volume 列表中，则该 Volume 会被隐式添加( implicitly added)。 If a volume with the name volumeName does not exist in the list of volumes that should be attached to this node, the volume is implicitly added. 3.3 findAndRemoveDeletedPods先从 desiredStateOfWorld 中遍历待挂载的 Volume，然后从 PodManager 中根据待挂载 Volume 的 Pod UID 查找该对应 Pod。跳过正在运行和不需要删除 Volumes（keepTerminatedPodVolumes）的 Pod，执行删除流程。 当 Pod 从 PodManager 中删除 Pod 时，Pod 不会在 Volume Manager 中立即删除，需要确认 kubelet 容器运行时所有的 Container 已经全部终止。此外，同时还要确认 actualStateOfWorld 缓存中是否存在待挂载 Volume 信息。 上述确认过程确认完毕后，从 desiredStateOfWorld 缓存中删除 Pod，表明指定的 Pod 不再需要该 Volume。同时，从 dswp 维护的 processedPods 列表中删除该 Pod。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"《基于容器的分布式系统设计模式》读书笔记","slug":"《基于容器的分布式系统设计模式》读书笔记","date":"2019-08-29T11:27:00.000Z","updated":"2021-04-28T13:39:23.038Z","comments":true,"path":"2019-08-29-dpfcds-notes/","link":"","permalink":"https://study4.fun/2019-08-29-dpfcds-notes/","excerpt":"","text":"设计模式的意义让缺少经验的开发者能够更容易地开发出可复用且高效稳定的程序 分类单容器单节点多节点","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"设计模式","slug":"设计模式","permalink":"https://study4.fun/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Kubernetes Java Client代码学习——list all pods","slug":"Kubernetes Java Client代码学习——list all pods","date":"2019-05-22T08:08:00.000Z","updated":"2021-04-28T13:39:23.250Z","comments":true,"path":"2019-05-22-list-all-pods/","link":"","permalink":"https://study4.fun/2019-05-22-list-all-pods/","excerpt":"","text":"从官方提供的代码入手，”列出所有 Pod”的示例代码如下： 123456789101112public class Example &#123; public static void main(String[] args) throws IOException, ApiException&#123; ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); for (V1Pod item : list.getItems()) &#123; System.out.println(item.getMetadata().getName()); &#125; &#125;&#125; 首先，给出一张手绘的流程图。 0x02. 核心流程2.1 创建 ApiClient 对象首先是创建 ApiClient 对象，从 defaultClient 方法切入，它返回的是一个由 ClientBuilder 的 standard 方法创建的 ApiClient 对象。 123public static ApiClient defaultClient() throws IOException &#123; return ClientBuilder.standard().build();&#125; 2.1.1 Standard 方法继续追踪 ClientBuilder 中 Standard 方法的源码，代码注释显示该方法会通过四种预先配置好的方式中的一种创建一个 builder，优先度顺序如下： 如果环境变量中 KUBECONFIG 定义过，则直接使用该配置； 如果$HOME/.kube/config可以被找到，则使用该配置； 如果In-cluster Service Account能被找到的话，则它就承担集群配置的功能； 上述都不存在，则默认使用localhost：8080作为最后的方法。 如果配置文件或者对应的配置无效的话，会抛出 ConnectException 异常。下来根据代码来验证以上的注释内容： 123456789101112131415161718192021222324252627282930313233public static ClientBuilder standard(boolean persistConfig) throws IOException &#123; #1.首先从环境变量中找ENV_KUBECONFIG final File kubeConfig = findConfigFromEnv(); #找到则读取对应配置文件，执行对应加载过程并返回 if (kubeConfig != null) &#123; try (FileReader kubeConfigReader = new FileReader(kubeConfig)) &#123; KubeConfig kc = KubeConfig.loadKubeConfig(kubeConfigReader); if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(kubeConfig)); &#125; return kubeconfig(kc); &#125; &#125; #2.其次从Home目录找.kube/config文件 final File config = findConfigInHomeDir(); //完成目录文件路径的拼接并返回File对象 if (config != null) &#123; try (FileReader configReader = new FileReader(config)) &#123; KubeConfig kc = KubeConfig.loadKubeConfig(configReader); if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(config)); &#125; return kubeconfig(kc); &#125; &#125; #3.采用集群内ServiceCount方式配置，这里传入的是SC对应的证书文件路径 final File clusterCa = new File(SERVICEACCOUNT_CA_PATH); if (clusterCa.exists()) &#123; return cluster(); &#125; #4.上述方式无效则使用默认构造器方法创建一个实例，实例中DEFAULT_FALLBACK_HOST指定的就是http://localhost:8080 # private String basePath = Config.DEFAULT_FALLBACK_HOST; return new ClientBuilder();&#125; 可以看到代码中实现与注释一一对应，在第一二种配置方式中，核心的方法主要有 KubeConfig 的 loadKubeConfig 和 setPersistConfig 两个方法。从字面上来看，前者主要负责配置加载，而后者则是对于 PersistConfig 的设置，具体是什么后面再看。 首先分析 loadKubeConfig 方法： 123456789101112131415161718192021public static KubeConfig loadKubeConfig(Reader input) &#123; #可以看到这里将输入读取为一个yaml对象， Yaml yaml = new Yaml(new SafeConstructor()); Object config = yaml.load(input); #然后将yaml形式的config对象转化为Map Map&lt;String, Object&gt; configMap = (Map&lt;String, Object&gt;) config; #通过转化为的map取出对应的五个值 String currentContext = (String) configMap.get(\"current-context\"); ArrayList&lt;Object&gt; contexts = (ArrayList&lt;Object&gt;) configMap.get(\"contexts\"); ArrayList&lt;Object&gt; clusters = (ArrayList&lt;Object&gt;) configMap.get(\"clusters\"); ArrayList&lt;Object&gt; users = (ArrayList&lt;Object&gt;) configMap.get(\"users\"); Object preferences = configMap.get(\"preferences\"); #将取出来的值装配到kubeconfig对象里，最后返回 KubeConfig kubeConfig = new KubeConfig(contexts, clusters, users); kubeConfig.setContext(currentContext); kubeConfig.setPreferences(preferences); return kubeConfig;&#125; 从 KubeConfig 类的变量声明能看到一些额外的信息： 123456789101112131415161718// 找到kubeconfig文件的默认地址相关字段public static final String ENV_HOME = \"HOME\";public static final String KUBEDIR = \".kube\";public static final String KUBECONFIG = \"config\";private static Map&lt;String, Authenticator&gt; authenticators = new HashMap&lt;&gt;();// 作者留下的注释//“致读者：我曾考虑过不使用多个Map，而是创建一个config对象并解析，但是使用多个Map要比一堆样板类更加清晰易懂”private ArrayList&lt;Object&gt; clusters;private ArrayList&lt;Object&gt; contexts;private ArrayList&lt;Object&gt; users;String currentContextName;Map&lt;String, Object&gt; currentContext;Map&lt;String, Object&gt; currentCluster;Map&lt;String, Object&gt; currentUser;String currentNamespace;Object preferences;ConfigPersister persister; standard 方法中，在执行完 loadKubeConfig 对象之后，会对传入的 persistConfig 标志位进行判断，如果为 true，则执行 setPersistConfig 方法： 123if (persistConfig) &#123; kc.setPersistConfig(new FilePersister(kubeConfig));&#125; 而 FilePersister 是 ConfigPersister 接口的一个具体实现，该接口仅有一个 save 方法，应当是将配置持久化的方法。 123456789public interface ConfigPersister &#123; public void save( ArrayList&lt;Object&gt; contexts, ArrayList&lt;Object&gt; clusters, ArrayList&lt;Object&gt; users, Object preferences, String currentContext) throws IOException;&#125; standard 方法最后的执行过程是 kubeconfig 方法，方法的注释说明该方法用于从一个预配置好的 KubeConfig 对象中创建 builder，具体源码如下： 123456789101112131415161718192021222324252627public static ClientBuilder kubeconfig(KubeConfig config) throws IOException &#123; final ClientBuilder builder = new ClientBuilder(); #拼装server字段 String server = config.getServer(); if (!server.contains(\"://\")) &#123; if (server.contains(\":443\")) &#123; server = \"https://\" + server; &#125; else &#123; server = \"http://\" + server; &#125; &#125; #根据证书取出KubeConfig的数据或者文件 final byte[] caBytes = KubeConfig.getDataOrFile( config.getCertificateAuthorityData(), config.getCertificateAuthorityFile()); #将kubeconfig对应的数据取出来填充进builder对象中 if (caBytes != null) &#123; builder.setCertificateAuthority(caBytes); &#125; builder.setVerifyingSsl(config.verifySSL()); builder.setBasePath(server); builder.setAuthentication(new KubeconfigAuthentication(config)); return builder;&#125; 2.1.2 Build 方法经历以上过程，终于 standard 方法执行完毕并返回了一个包含各种认证鉴权相关信息的 builder 对象，接下来看看 build 方法的实现。 1234567891011121314151617181920212223public ApiClient build() &#123; final ApiClient client = new ApiClient(); if (basePath != null) &#123; if (basePath.endsWith(\"/\")) &#123; basePath = basePath.substring(0, basePath.length() - 1); &#125; client.setBasePath(basePath); &#125; client.setVerifyingSsl(verifyingSsl); if (authentication != null) &#123; authentication.provide(client); &#125; //上述顺序很重要，因为一旦SSL信息更改，API客户端就会重新评估CA证书。这意味着如果上述流程发生在下面这个调用的后面，在尝试加载证书时，很可能会调用的InputStream已经耗尽。 因此，设置CA证书的顺序必须位于最后。 if (caCertBytes != null) &#123; client.setSslCaCert(new ByteArrayInputStream(caCertBytes)); &#125; return client;&#125; 2.2 剩余工作经历以上步骤，终于完成了 ApiClient 复杂的创建过程。继续看看看看剩余工作，如下： 1234567891011#创建client对象ApiClient client = Config.defaultClient();#将client对象传入到Configuration类的类静态变量中Configuration.setDefaultApiClient(client);#CoreV1Api无参构造方法调用重载的带apiClient参数的构造方法，从Configuration类中取出client对象完成构造CoreV1Api api = new CoreV1Api();#获得CoreV1Api对象后，填充参数执行对应的查询方法V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); 0x03. 总结流程经历以上的分析过程，大致完成了 list pods 的流程分析。结合 K8S 官方的概念流程文档可以验证之前学习的基于 API Server 的安全机制流程，可以看到 Authentication 和 Authorization 部分的数据填充过程。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"}]},{"title":"Go学习笔记——基础知识","slug":"Go学习笔记——基础知识","date":"2019-05-17T09:32:00.000Z","updated":"2021-04-28T13:39:23.358Z","comments":true,"path":"2019-05-17-go-basic/","link":"","permalink":"https://study4.fun/2019-05-17-go-basic/","excerpt":"","text":"特点 静态语言，支持运行时动态类型；支持隐式类型推导； 接口基于Duck 模型；仅通过接口支持多态；不支持泛型但支持反射； 编译成可执行程序直接执行；支持自动垃圾回收；语言原生支持并发；跨平台；多应用于云计算基础设施软件、中间件、区块链等。 程序结构123456789# 文件名：hello.go# 包名，main为可执行程序的包名package main# 引入外部包fmt（标准输入输出）import \"fmt\"# 函数声明，main代表程序入口函数func main()&#123; fmt.Printf(\"Hello, world.\\n\")&#125; 补充： Go 的源程序默认使用 UTF-8 编码 语句结尾的分号可以省略 “{”必须在函数头所在行尾部，不能单独起一行 main 函数所在的包名必须是 main。 编译运行 编译 go build hello.go 运行 ./hello 标识符（仅记录与 Java 不同的） 关键字常量声明 const、变量定义 var、函数定义 func、延迟执行 defer、 结构类型定义 struct、通道类型 chan 数据类型标识 类型 标识 整型 byte int int8-int64 uint uint8-uint64 uintptr （byte 就是 uint8） 浮点数 float32 float64 （自动类型推断为 float64） 复数 complex64 complex128 （由两个 float 构成，对应实部和虚部） 字符 rune 接口 error 连续枚举类型 iota 匿名变量 _ iota 用法: iota 用于常量声明中，初始值为 0，逐行增加 123456const ( a = iota //iota=0,a=0 b //iota=1,b=1 c = 3 //iota=2,未使用，c=3 d = 1 &lt;&lt; iota //iota=3,d=8) 注意：Go 语言里自增和自减是语句而不是表达式[1] 变量和常量变量声明1234567//显式声明，value可以是表达式，不指定则初始化为类型零值，声明后立即分配空间var varName dataType [ = value]//短类型声明，只能出现在函数内，自动进行数据类型推断varName := value//支持多个类型变量同时声明并赋值a, b := 1, \"hello\" 字符串和切片 字符串可以通过类似数组索引的方式访问，但是不能修改 字符串转换为切片[]byte()在数据量大的时候要慎用，因为转换的时候需要复制内容 字符串的底层实现是一个指向字节数组的指针和字节数组长度 基于字符串创建的切片指向原字符串指向的字符数组，不可修改 指针 结构体指针访问结构体字段仍然使用”.”操作符，没有“-&gt;” 不支持指针运算（GO 支持垃圾回收，语言层面禁止指针运算） 允许返回局部变量地址 切片切片（可变数组）维护三个元素——指向底层数组的指正、切片元素数量、底层数组容量 创建方式：数组索引、make MapGo 内置的 map 不是并发安全的，需要时用 sync 包内的 map 保证并发安全 map 键值对的修改不能通过 map 引用直接修改键值，需要 KV 整体赋值 控制结构If-else123if initialization; condition &#123; // do something&#125; Switch 条件表达式支持任意支持相等比较运算的类型变量 switch 后面可以带上初始化语句 case 后可以使用多个值比较，使用逗号分隔 配合使用可以进行类型查询 Goto goto 需要配合标签使用 goto 只能在函数内跳转，但是不能跳过内部变量声明语句，只能跳过同级作用域或上层作用域 Q：【1】这是否意味着自增或自减是原子操作？答：不是。","categories":[{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://study4.fun/tags/Golang/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Kubernetes学习笔记——基于API Server的安全机制浅析","slug":"Kubernetes学习笔记——基于API Server的安全机制浅析","date":"2019-04-25T02:50:00.000Z","updated":"2021-04-28T13:39:23.310Z","comments":true,"path":"2019-04-25-k8s-api-server-security/","link":"","permalink":"https://study4.fun/2019-04-25-k8s-api-server-security/","excerpt":"","text":"集群安全机制的目标 隔离性，限制容器给集群带来的副作用 最小权限原则 组件的边界划分需要明确 角色划分和权限分配 关于 API ServerAPI Server 作为集群控制请求的实际入口，通常暴露了两个端口——本地端口(Localhost Por)和安全端口(Secure Port)。 本地端口 安全端口 使用场景 测试和启动时使用，Master 节点中不同组件通信 任意场景 安全协议 无 TLS 端口 默认 8080， insecure-port修改 | 默认 6443，secure-port修改 || IP | 默认 localhost,insecure-bind-address修改 | 默认为第一个非 localhost 网卡地址，bind-address修改 || 处理流程 | 无需认证和授权 | 需要认证和授权 || 准入控制 | 是 | 是 || 访问控制 | 需要拥有主机访问权限 | 需要认证和授权模块正常运行 | 注：TLS 中，证书和私钥相关参数为tls-cert-file 和 tls-private-key-file。访问 API Server 的方式有 kubectl，客户端的库和 Rest 请求。通常来说，想在外部访问 API Server 需要通过安全端口访问。通过安全端口访问需要经过三重校验，即 Authentication（身份认证），Authorization（授权）和 Admission Control（准入控制）。 Authentication（身份认证） Authentication is the act of confirming the truth of an attribute of a single piece of data claimed true by an entity.From: https://en.wikipedia.org/wiki/Authentication通俗的来说，身份认证解决的是“让系统/服务端知道你是谁”的问题，即对用户身份的确认。值得注意的是，Wikipedia 中还特别标注“ Not to be confused with authorization. ”对于身份的认证，现实生活中可能是查验证件，也可能是对暗号。 对应的，在 Kubernetes 中，也有对应的几种客户端身份认证方式： 证书认证：基于 CA 根证书签名的双向数字证书认证方式； Token 认证：通过 Token 来认证； 用户密码：通过用户密码的方式认证； 可以同时指定多个身份认证模块，在流程中将会以顺序执行的方式进行认证过程，直到其中一个认证模块认证成功。如果请求认证失败，则会返回 401 状态码。（PS：这里引入了一个 401 状态码的历史遗留问题——401 的语义其实应该是 Unauthenticated） 一旦认证成功，用户就会被分配一个特定的 username。在随后的访问控制流程中，这个 username 将会一直使用。尽管如此，这个 username 却也不会对应存在一个真实的用户对象，该信息也不会被存储。 （我的理解：这个所谓的 username 的存在仅仅是为了在整个访问控制流程中能够进行上下文信息的传递，完成一个链式的验证。和传统意义上的用户相比，K8S 的访问控制基于单次的请求，在请求的过程中抽象来决定行为的合法性和有效性。） Authorization（授权） Authorization is the function of specifying access rights/privileges to resources, which is related to information security and computer security in general and to access control in particular.From: https://en.wikipedia.org/wiki/Authorization与身份认证不同的是，授权关注的是对资源的访问控制，通俗的说就是“系统要知道你这个身份能够做什么”。 当请求通过了身份认证之后，请求才会进入授权流程。请求的内容包含三个部分：用户名（username of the requester），动作（ requested action），动作影响的对象（the object affected by the action）。在已有的多种授权策略中，只要有一个能够声明此用户有执行对应动作的权限，则请求就被授权成功。若所有授权策略全部失败，则返回 403 状态码。授权模块的种类： ABAC RBAC Node Webhook Admission Control（准入控制）通过认证和授权流程之后，请求的调用还需要通过准入控制链的检查。与上述模块不同，准入控制能够修改请求参数完成一些任务。当多个准入控制模块配置完毕后，请求的调用会依次按顺序进行检查。一旦任意一个准入控制模块检查不通过，则请求立即被拒绝。而请求完成了所有检查后，会采用相应 API 对象的验证流程对请求进行验证，然后写入对象库。 Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store (shown as step 4). 总结从整体来看，Kubernetes 对于 API Server 的请求主要分为两大部分：内部和外部。内部是指在 master 节点使用 kubectl 命令进行操作，这时，因为是在节点内部操作，因此并不会使用安全端口，直接采用 localhost 这个 ip 上的非安全端口进行访问。而对于 API Server 的外部请求调用（包括 Pod 和 Rest 请求），则需要使用安全端口进行访问。通过安全端口的请求，需要进行严格的三层校验才能调用成功。这就通过确保只执行权限内允许的操作保证了集群操作的安全性。关于每个部分的详细介绍，会单独抽成三部分继续分解。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"API Server","slug":"API-Server","permalink":"https://study4.fun/tags/API-Server/"}]},{"title":"校园网下OpenWrt配置DNS","slug":"校园网下OpenWrt配置DNS","date":"2018-09-09T05:55:00.000Z","updated":"2021-04-28T13:39:23.514Z","comments":true,"path":"2018-09-09-config-dns/","link":"","permalink":"https://study4.fun/2018-09-09-config-dns/","excerpt":"","text":"在上一篇文章的末尾写到，初次配置 OP 系统时候在校内网实现了 IPV6 穿透，但是 DNS 出现了问题。查阅资料后发现与 OP 的反域名劫持保护机制有关系，下面详述一下。 0x02 OP 的反域名劫持保护OP 的反域名劫持保护在默认情况是开开启的，具体设置在/etc/config/dhcp下。 12config dnsmasq option rebind_protection &#39;1&#39; 在反域名劫持保护未关闭的情况下，由于上级 dns 返回的地址是个私有局域网地址，所以被看作是一次域名劫持，从而丢弃了解析的结果。 直接的方法就是将上面的字段值改为0，关闭即可。在 GUI 配置界面等同于将Network-&gt;DHCP DNS-Server Settings-&gt;General Settings-&gt;Rebind protection的勾取消掉。 再仔细查看文档发现也可以通过白名单的方式放行想要解析的内网域名，更为安全,此时 Rebind protection 也是处于开启状态，上面的关闭操作不需要进行。而具体修改的操作示例如下所示： 123config dnsmasq list rebind_domain &#39;bupt.edu.cn&#39; list rebind_domain &#39;byr.cn&#39; 表示在反域名劫持保护情况下，将bupt.edu.cn,byr.cn域名加入白名单，允许返回内网地址。在 GUI 配置界面等同于在Network-DHCP DNS-Server Settings-General Settings-Domain whitelist添加想要解析的内网域名。 0x03 自定义 DNS 规则在学校 DNS 偶尔抽风或者速度慢的情况下，产生了自定义 DNS 的想法。由于教务系统等系统的访问需要，在各个客户端修改 hosts 略显麻烦，并且 DNSmasq 亦可以实现广告屏蔽，因此采用 DNSmasq 来实现不同的 DNS 解析。预期需求为bupt.edu.cn, byr.cn域名使用校内 DNS 解析，其他地址使用公共 DNS 解析（以 114 为例）。 修改 Wan 口 DNSWan 口 DNS 主要控制路由器访问网络使用的 DNS 服务器。例如，路由器安装软件需要访问网络，那么所使用的 DNS 服务器就是这个。 在/etc/config/network文件中的 wan 接口添加两行 peerdns 以及 dns 字段： 12345config interface &#39;wan&#39; ...... option peerdns &#39;0&#39; option dns &#39;114.114.114.114&#39; ...... 重启 network 服务后生效。在 GUI 配置界面等同于在Network-Interface-Wan-Edit-Common Configuration-Advanced Settings中取消Use DNS servers advertised by peer的勾选，并在Use custom DNS servers添加默认的 DNS 服务器。 修改 Lan 口 DNS（可跳过）LAN 口 DNS 主要控制连接到路由器的设备使用的 DNS。例如，连到路由的电脑上网时使用的 DNS 服务器就在这里设置。 一般情况下，Lan 和 Wan 口 DNS 保持一致即可。如若有需要，修改/etc/config/dhcp文件中 dnsmasq 的 resolvfile 指向即可： 1234config dnsmasq .... option resolvfile &#39;&#x2F;etc&#x2F;resolv.dnsmasq.conf&#39; .... 同时需要在/etc/resolv.dnsmasq.conf下新建对应的配置文件。示例如下： 12nameserver 114.114.114.114nameserver 2001:4860:4860::8888 修改 Dns 自定义的内网解析规则接下来就是配置校内域名使用的 DNS 解析地址。修改/etc/config/dhcp文件中 dnsmasq。首先是删除下面两行配置： 12345config dnsmasq .... option filterwin2k &#39;0&#39; option nonegcache &#39;0&#39; .... 其次添加 list server 字段，对bupt.edu.cn, byr.cn相关域名使用校内 DNS 解析： 123config dnsmasq list server &#39;&#x2F;bupt.edu.cn&#x2F;10.3.9.5&#39; list server &#39;&#x2F;byr.cn&#x2F;10.3.9.5&#39; 在 GUI 配置界面等同于在Network-DHCP and DNS-Server Settings-General Settings-DNS forwardings添加对应域名的 DNS 解析服务器地址。 0x04 后话DNSmasq 的用法远不止于此，可用来内网域名 IP 映射代替 hosts，自定义域名解析规则屏蔽广告等，有时间会再研究。","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"DNS","slug":"DNS","permalink":"https://study4.fun/tags/DNS/"}]},{"title":"Kubernetes学习笔记——Pod调度","slug":"Kubernetes学习笔记——Pod调度","date":"2018-09-09T05:55:00.000Z","updated":"2021-04-28T13:39:23.462Z","comments":true,"path":"2018-09-09-k8s-pod-scheduling/","link":"","permalink":"https://study4.fun/2018-09-09-k8s-pod-scheduling/","excerpt":"","text":"0x01. Deployment/RC 全自动调度效果：在集群内始终维持用户指定的副本数量 使用：spec.replicas 原理：系统自动调度算法。由 Master 的 Scheduler 经过一系列算法计算得出，用户无法干预调度过程和结果。 0x02. NodeSelector效果：通过 Node 的标签和 Pod 的 nodeSelector 属性进行匹配，将 Pod 调度到指定的 Node 上 使用： 为目标 Node 打标签 1kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; 在 Pod 定义中加上 nodeSelector 的设置 1234#pod.yaml---nodeSelector: &lt;label-key&gt;: &lt;your-selected-label-name&gt; 补充： 如果多个 Node 定义了相同的标签，则会根据调度算法从这组 Node 中挑选一个可用的 Node 进行调度 若 Pod 指定的 nodeSelector 条件集群中不存在符合的节点，则该 Pod 无法被成功调度，即使集群中还有可用的 Node 0x03. 亲和性调度篇幅原因，另外一篇单独记录 0x04. 污点(Taints)和容忍(Tolerations)效果：Pod 无法在标记了 Taint 属性的节点上运行, 同时，设置了 Tolerations 的 Pod 可以运行在标注了 Taint 的 Node 上 使用： 为 Node 设置 Taint 信息 1kubectl taint nodes node1 key1=value1:NoSchedule 在 Pod 的配置文件中配置 tolerations 属性 12345tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchdule\" 补充： Taint 和 Toleration 声明需要保持对应一致，且 operator 需要为 Exists 或者 Equal（Equal 需要指定相等 value）； 空 key 配合 Exists 能够匹配所有键值，空 effect 匹配所有 effect； effect 取值也可以设置为 Prefer，例如 PreferNoSchedule，视为软限制； 同个 Node 可以设置多个 Taint，对应的，Pod 也可以设置多个 Toleration。 0x05. DaemonSet效果：在每个 Node 上调度运行同一个（种）Pod，例如日志采集、性能监控、存储的 Daemon 进程 使用： 12apiVersion: extensions/v1beta1kind: DaemonSet 补充： 除了使用系统内置算法在每台 Node 上调度外，也可以在 Pod 定义中使用 NodeSelector 或者 NodeAffinity 来指定满足料件的 Node 范围进行调度 0x06. 批处理调度效果：并行/串行启动多个计算进程去处理一批工作项（Work Item，下称 WI），处理完成后，批处理任务结束 任务模式分类： Job Template Expansion 模式：Job 和 WI 一对一对应，适用于 WI 数量少，但是单 WI 处理数据量大的场景； Queue with Pod Per Work Item 模式：使用一个任务队列存放 WI，Job 作为 Consumer 去完成 WI（对应的，Job 会启动多个 Pod，每个 Pod 对应一个 WI），可用 MQ 实现； Queue with Variable Pod Count 模式：与 2 模式类似，但是 Job 启动的 Pod 数量是可变的，可用 Redis 或数据库实现； Single Job with Static Work Assignment 模式：一个 Job 产生多个 Pod，但是采用程序静态方式分配任务（Kubernetes 不支持，书中所写）。 Job 分类： Non-parallel Jobs：一个 Job 启动一个 Pod，Pod 正常结束则 Job 结束。 Parallel Jobs with a fixed completion count：Job 会启动多个 Pod（数目为 spec.completions），正常结束的 Pod 达到这个数目后 Job 结束。spec.parallelism 可以用来控制并行度。 Parallel Jobs with a work queue：WI 在 Queue 中存放，无法设置并行度参数。每个 Pod 都能够独立判断是否还有任务需要处理，同时，一个 Pod 成功结束则其他 Pod 必定处于即将结束、退出的状态，且 Job 不会再启动新的 Po）。所有 Pod 结束，且至少一个 Pod 成功结束则 Job 算成功结束。 （个人理解：上述的规则说明其实是在说所有 Pod 表现为同一整体，Pod 启动失败会重启是一种容错机制。然而从整个过程的跨度来看，无需关心失败启动的数目，只要不是所有 Pod 全部失败结束，只需存在一个成功结束的 Pod 即表明 Job 流程内的其他划分任务都正常完成，整体任务也已成功完成。） 0x07. 定时任务效果：定期触发任务执行 使用： 在 API Server 启动进程上添加配置参数 1--runtime-config=batch/v2alpha1=true 编写 Cron Job 配置文件 123456#cron.yamlapiVersion: batch/v2alpha1kind: CronJob---spec: schedule: \"*/1 * * * *\" schedule 格式如下 1Min Hour DayOfMonth Month DayOfWeek *表示任意值，即每个时间单元节点都会触发 /表示开始触发的时间，例如 5/20，表明第一次触发在第 5 个时间单位，此后每隔 20 个时间单位触发 0x08. 自定义调度器在 Pod 中提供自定义的调度器名称，则默认调度器就会失效，转而使用指定的调度器完成对应 Pod 的调度，自定义的调度器需要通过 kube-proxy 来运行，如果自定义调度器始终未启动，则 Pod 将会卡 Pending 状态。 12345apiVersion: v1kind: Pod---spec: schedulerName: my-scheduler 0x09. 补充 Admission controller 需要仔细研究 TaintBasedEviction 和 Eureka 中的驱逐机制（包括 SELF PRESERVATION)是否在设计层面上有一定的共通点 自定义调度器实现有时间需要手动验证一次","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Pod","slug":"Pod","permalink":"https://study4.fun/tags/Pod/"}]},{"title":"双栈网络配置路由器的ipv6穿透","slug":"双栈网络配置路由器的ipv6穿透","date":"2016-03-20T14:47:00.000Z","updated":"2021-04-28T13:39:23.558Z","comments":true,"path":"2016-03-20-config-ipv6/","link":"","permalink":"https://study4.fun/2016-03-20-config-ipv6/","excerpt":"","text":"本教程测试过程基于 Newifi Y2 路由器，系统为 PandoraBox，理论上 OpenWrt 原生同样适用。 1.修改路由器的软件源123456789101112dest root &#x2F;dest ram &#x2F;tmplists_dir ext &#x2F;var&#x2F;opkg-listsoption overlay_root &#x2F;overlaysrc&#x2F;gz 14.09_base http:&#x2F;&#x2F;downloads.openwrt.org.cn&#x2F;PandoraBox&#x2F;ralink&#x2F;packages&#x2F;basesrc&#x2F;gz 14.09_telephony http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;telephonysrc&#x2F;gz 14.09_packages http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;packagessrc&#x2F;gz 14.09_routing http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;routingsrc&#x2F;gz 14.09_management http:&#x2F;&#x2F;downloads.openwrt.org&#x2F;chaos_calmer&#x2F;15.05&#x2F;ramips&#x2F;mt7620&#x2F;packages&#x2F;managementarch ralink 1arch all 2arch ramips_24kec 3 注:软件源由于硬件配置不同的会有所区别，Newifi 是 MT7620 方案，其他芯片方案的请移步以下两个网址自行匹配： OpenWrt 中文网址 http://downloads.openwrt.org.cn/OpenWrt download area https://downloads.openwrt.org/ 2.配置 Odhcpd 软件包中更新并安装最新版本的 odhcpd 修改 dhcp 文件（文件路径：/etc/config/dhcp，不熟悉 ssh 的可以使用 winscp 修改) 123456789101112131415161718192021222324252627282930313233343536373839404142config dnsmasq option domainneeded &#39;1&#39; option boguspriv &#39;1&#39; option filterwin2k &#39;0&#39; option localise_queries &#39;1&#39; option rebind_protection &#39;1&#39; option rebind_localhost &#39;1&#39; option local &#39;&#x2F;lan&#x2F;&#39; option domain &#39;lan&#39; option expandhosts &#39;1&#39; option nonegcache &#39;0&#39; option authoritative &#39;1&#39; option readethers &#39;1&#39; option leasefile &#39;&#x2F;tmp&#x2F;dhcp.leases&#39; option resolvfile &#39;&#x2F;tmp&#x2F;resolv.conf.auto&#39; option localservice &#39;1&#39;config dhcp &#39;lan&#39; option interface &#39;lan&#39; option start &#39;100&#39; option limit &#39;150&#39; option leasetime &#39;12h&#39; option dhcpv6 &#39;hybrid&#39; option ra &#39;hybrid&#39; option ndp &#39;hybrid&#39; option ra_management &#39;1&#39;config dhcp &#39;wan&#39; option interface &#39;wan&#39; option ignore &#39;1&#39;config odhcpd &#39;odhcpd&#39; option maindhcp &#39;0&#39; option leasefile &#39;&#x2F;tmp&#x2F;hosts&#x2F;odhcpd&#39; option leasetrigger &#39;&#x2F;usr&#x2F;sbin&#x2F;odhcpd-update&#39;config dhcp &#39;wan6&#39; option interface &#39;wan&#39; option dhcpv6 &#39;hybrid&#39; option ra &#39;hybrid&#39; option ndp &#39;hybrid&#39; option master &#39;1&#39; 修改后保存并重启路由器即可。 3.后话配置后好像 dns 出了一些问题，在访问其他校内以.byr.cn 或.bupt.edu.cn 为后缀的网址显示 dns 错误，如果有大牛解决了这个 DNS 问题，可以分享一下思路。 2018.9.9 更新：后话所述问题已经解决","categories":[{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"IPV6","slug":"IPV6","permalink":"https://study4.fun/tags/IPV6/"}]}],"categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/categories/%E6%9D%82%E9%A1%B9/"},{"name":"备忘录","slug":"备忘录","permalink":"https://study4.fun/categories/%E5%A4%87%E5%BF%98%E5%BD%95/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"日记","slug":"日记","permalink":"https://study4.fun/categories/%E6%97%A5%E8%AE%B0/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://study4.fun/tags/Kubernetes/"},{"name":"Kubelet","slug":"Kubelet","permalink":"https://study4.fun/tags/Kubelet/"},{"name":"杂项","slug":"杂项","permalink":"https://study4.fun/tags/%E6%9D%82%E9%A1%B9/"},{"name":"容器","slug":"容器","permalink":"https://study4.fun/tags/%E5%AE%B9%E5%99%A8/"},{"name":"cgroup","slug":"cgroup","permalink":"https://study4.fun/tags/cgroup/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://study4.fun/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Unix","slug":"Unix","permalink":"https://study4.fun/tags/Unix/"},{"name":"学习笔记","slug":"学习笔记","permalink":"https://study4.fun/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"设计模式","slug":"设计模式","permalink":"https://study4.fun/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Golang","slug":"Golang","permalink":"https://study4.fun/tags/Golang/"},{"name":"API Server","slug":"API-Server","permalink":"https://study4.fun/tags/API-Server/"},{"name":"网络配置","slug":"网络配置","permalink":"https://study4.fun/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"DNS","slug":"DNS","permalink":"https://study4.fun/tags/DNS/"},{"name":"Pod","slug":"Pod","permalink":"https://study4.fun/tags/Pod/"},{"name":"IPV6","slug":"IPV6","permalink":"https://study4.fun/tags/IPV6/"}]}